{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from ipywidgets import widgets\n",
    "import numpy as np\n",
    "import math\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from torchvision.transforms import Compose, CenterCrop, ToTensor, Scale, RandomCrop, RandomHorizontalFlip\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "from fastai2.vision.widgets import *\n",
    "from fastai2.vision.all import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LapSRN single image super-resolution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Conv_Block(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(_Conv_Block, self).__init__()\n",
    "        \n",
    "        self.cov_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):  \n",
    "        output = self.cov_block(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv_input = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        self.convt_I1 = nn.ConvTranspose2d(in_channels=3, out_channels=3, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.convt_R1 = nn.Conv2d(in_channels=64, out_channels=3, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.convt_F1 = self.make_layer(_Conv_Block)\n",
    "  \n",
    "        self.convt_I2 = nn.ConvTranspose2d(in_channels=3, out_channels=3, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.convt_R2 = nn.Conv2d(in_channels=64, out_channels=3, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.convt_F2 = self.make_layer(_Conv_Block)        \n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                c1, c2, h, w = m.weight.data.size()\n",
    "                weight = get_upsample_filter(h)\n",
    "                m.weight.data = weight.view(1, 1, h, w).repeat(c1, c2, 1, 1)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "                    \n",
    "    def make_layer(self, block):\n",
    "        layers = []\n",
    "        layers.append(block())\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):    \n",
    "\n",
    "        out = self.relu(self.conv_input(x))\n",
    "        \n",
    "        convt_F1 = self.convt_F1(out)\n",
    "        convt_I1 = self.convt_I1(x)\n",
    "        convt_R1 = self.convt_R1(convt_F1)\n",
    "        HR_2x = convt_I1 + convt_R1\n",
    "        \n",
    "        \n",
    "        convt_F2 = self.convt_F2(convt_F1)\n",
    "        convt_I2 = self.convt_I2(HR_2x)\n",
    "        convt_R2 = self.convt_R2(convt_F2)\n",
    "        HR_4x = convt_I2 + convt_R2\n",
    "        \n",
    "        return HR_4x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_upsample_filter(size):\n",
    "    \"\"\"Make a 2D bilinear kernel suitable for upsampling\"\"\"\n",
    "    factor = (size + 1) // 2\n",
    "    if size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = np.ogrid[:size, :size]\n",
    "    filter = (1 - abs(og[0] - center) / factor) * \\\n",
    "             (1 - abs(og[1] - center) / factor)\n",
    "    return torch.from_numpy(filter).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('model.pth', map_location = torch.device('cpu'))\n",
    "model = Net()\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def superresolve(img_tensor):\n",
    "    print('This is the image being fed into the model')\n",
    "    img_pil = torchvision.transforms.ToPILImage()(img_tensor.squeeze(0))\n",
    "    plt.imshow(img_pil)\n",
    "    plt.show()\n",
    "    output = model(img_tensor)\n",
    "    output4x = torchvision.transforms.ToPILImage()(output.squeeze_(0))\n",
    "    print('This is the output from the model')\n",
    "    plt.imshow(output4x)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_upload = widgets.FileUpload()\n",
    "out_pl = widgets.Output()\n",
    "lbl_pred = widgets.Label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_click(change):\n",
    "    img = PILImage.create(btn_upload.data[-1])\n",
    "    out_pl.clear_output()\n",
    "    with out_pl:\n",
    "        display(img.to_thumb(128,128))\n",
    "\n",
    "        my_transforms = torchvision.transforms.Compose([torchvision.transforms.Resize(128)\n",
    "                                    ,torchvision.transforms.CenterCrop(128)\n",
    "                                    ,torchvision.transforms.ToTensor()])\n",
    "        img_tensor = my_transforms(img)\n",
    "        img_tensor.unsqueeze_(0)\n",
    "        superresolve(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_upload.observe(on_click, names=['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(VBox([widgets.Label('Select your image'), btn_upload, out_pl, lbl_pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}